{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from fastparquet import ParquetFile\n",
    "import pyarrow \n",
    "import pyarrow.parquet as pq \n",
    "import wget \n",
    "import os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = 'yellow_tripdata_2024.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Set the output file path in the current directory\n",
    "output_file_path = os.path.join(current_dir, \"yellow_tripdata_2024.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/pawanchahar/Data-Engineering-Zoomcamp/Week_1_basics_n_setup/2_docker_sql/yellow_tripdata_2024 (1).parquet'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wget.download(url , output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>Airport_fee</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-01-01 00:57:55</td>\n",
       "      <td>2024-01-01 01:17:43</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.72</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>186</td>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>17.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.70</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 00:03:00</td>\n",
       "      <td>2024-01-01 00:09:36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>140</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.75</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 00:17:06</td>\n",
       "      <td>2024-01-01 00:35:01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>236</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>23.3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.30</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 00:36:38</td>\n",
       "      <td>2024-01-01 00:44:56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>79</td>\n",
       "      <td>211</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 00:46:51</td>\n",
       "      <td>2024-01-01 00:52:57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>211</td>\n",
       "      <td>148</td>\n",
       "      <td>1</td>\n",
       "      <td>7.9</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.10</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 00:54:08</td>\n",
       "      <td>2024-01-01 01:26:31</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>148</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>29.6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-01-01 00:49:44</td>\n",
       "      <td>2024-01-01 01:15:47</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>138</td>\n",
       "      <td>181</td>\n",
       "      <td>1</td>\n",
       "      <td>45.7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-01 00:30:40</td>\n",
       "      <td>2024-01-01 00:58:40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>246</td>\n",
       "      <td>231</td>\n",
       "      <td>2</td>\n",
       "      <td>25.4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.40</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-01-01 00:26:01</td>\n",
       "      <td>2024-01-01 00:54:12</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>161</td>\n",
       "      <td>261</td>\n",
       "      <td>2</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-01-01 00:28:08</td>\n",
       "      <td>2024-01-01 00:29:16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>113</td>\n",
       "      <td>113</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         2  2024-01-01 00:57:55   2024-01-01 01:17:43              1.0   \n",
       "1         1  2024-01-01 00:03:00   2024-01-01 00:09:36              1.0   \n",
       "2         1  2024-01-01 00:17:06   2024-01-01 00:35:01              1.0   \n",
       "3         1  2024-01-01 00:36:38   2024-01-01 00:44:56              1.0   \n",
       "4         1  2024-01-01 00:46:51   2024-01-01 00:52:57              1.0   \n",
       "5         1  2024-01-01 00:54:08   2024-01-01 01:26:31              1.0   \n",
       "6         2  2024-01-01 00:49:44   2024-01-01 01:15:47              2.0   \n",
       "7         1  2024-01-01 00:30:40   2024-01-01 00:58:40              0.0   \n",
       "8         2  2024-01-01 00:26:01   2024-01-01 00:54:12              1.0   \n",
       "9         2  2024-01-01 00:28:08   2024-01-01 00:29:16              1.0   \n",
       "\n",
       "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
       "0           1.72         1.0                  N           186            79   \n",
       "1           1.80         1.0                  N           140           236   \n",
       "2           4.70         1.0                  N           236            79   \n",
       "3           1.40         1.0                  N            79           211   \n",
       "4           0.80         1.0                  N           211           148   \n",
       "5           4.70         1.0                  N           148           141   \n",
       "6          10.82         1.0                  N           138           181   \n",
       "7           3.00         1.0                  N           246           231   \n",
       "8           5.44         1.0                  N           161           261   \n",
       "9           0.04         1.0                  N           113           113   \n",
       "\n",
       "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0             2         17.7    1.0      0.5        0.00           0.0   \n",
       "1             1         10.0    3.5      0.5        3.75           0.0   \n",
       "2             1         23.3    3.5      0.5        3.00           0.0   \n",
       "3             1         10.0    3.5      0.5        2.00           0.0   \n",
       "4             1          7.9    3.5      0.5        3.20           0.0   \n",
       "5             1         29.6    3.5      0.5        6.90           0.0   \n",
       "6             1         45.7    6.0      0.5       10.00           0.0   \n",
       "7             2         25.4    3.5      0.5        0.00           0.0   \n",
       "8             2         31.0    1.0      0.5        0.00           0.0   \n",
       "9             2          3.0    1.0      0.5        0.00           0.0   \n",
       "\n",
       "   improvement_surcharge  total_amount  congestion_surcharge  Airport_fee  \n",
       "0                    1.0         22.70                   2.5         0.00  \n",
       "1                    1.0         18.75                   2.5         0.00  \n",
       "2                    1.0         31.30                   2.5         0.00  \n",
       "3                    1.0         17.00                   2.5         0.00  \n",
       "4                    1.0         16.10                   2.5         0.00  \n",
       "5                    1.0         41.50                   2.5         0.00  \n",
       "6                    1.0         64.95                   0.0         1.75  \n",
       "7                    1.0         30.40                   2.5         0.00  \n",
       "8                    1.0         36.00                   2.5         0.00  \n",
       "9                    1.0          8.00                   2.5         0.00  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the parquet file directly with pandas\n",
    "df = pd.read_parquet(output_file_path)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('yellow_tripdata_2024-01.csv' ,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iter = pd.read_csv('yellow_tripdata_2024-01.csv' , iterator=True ,chunksize=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = next(df_iter)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=0).to_sql(name='yellow_taxi_data' ,con=engine , if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time  df.to_sql(name='yellow_taxi_data' ,con=engine , if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted another chunk........, took 13.566 seconds\n",
      "Inserted another chunk........, took 14.449 seconds\n",
      "Inserted another chunk........, took 13.016 seconds\n",
      "Inserted another chunk........, took 12.674 seconds\n",
      "Inserted another chunk........, took 12.879 seconds\n",
      "Inserted another chunk........, took 12.885 seconds\n",
      "Inserted another chunk........, took 13.400 seconds\n",
      "Inserted another chunk........, took 13.194 seconds\n",
      "Inserted another chunk........, took 14.768 seconds\n",
      "Inserted another chunk........, took 14.004 seconds\n",
      "Inserted another chunk........, took 14.149 seconds\n",
      "Inserted another chunk........, took 13.705 seconds\n",
      "Inserted another chunk........, took 14.135 seconds\n",
      "Inserted another chunk........, took 14.224 seconds\n",
      "Inserted another chunk........, took 13.419 seconds\n",
      "Inserted another chunk........, took 13.469 seconds\n",
      "Inserted another chunk........, took 13.780 seconds\n",
      "Inserted another chunk........, took 16.320 seconds\n",
      "Inserted another chunk........, took 13.062 seconds\n",
      "Inserted another chunk........, took 13.152 seconds\n",
      "Inserted another chunk........, took 13.696 seconds\n",
      "Inserted another chunk........, took 15.310 seconds\n",
      "Inserted another chunk........, took 18.189 seconds\n",
      "Inserted another chunk........, took 14.277 seconds\n",
      "Inserted another chunk........, took 13.875 seconds\n",
      "Inserted another chunk........, took 30.134 seconds\n",
      "Inserted another chunk........, took 16.061 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kp/_4404mkx1zn_85zd51sfg3440000gn/T/ipykernel_4367/2507646675.py:4: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = next(df_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted another chunk........, took 12.140 seconds\n",
      "Inserted another chunk........, took 7.974 seconds\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     t_start \u001b[39m=\u001b[39m time()\n\u001b[0;32m----> 4\u001b[0m     df \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(df_iter)\n\u001b[1;32m      7\u001b[0m     df\u001b[39m.\u001b[39mto_sql(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39myellow_taxi_data\u001b[39m\u001b[39m'\u001b[39m ,con\u001b[39m=\u001b[39mengine , if_exists\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mappend\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m     t_end \u001b[39m=\u001b[39m time()\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1843\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1841\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m   1842\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1843\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_chunk()\n\u001b[1;32m   1844\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m   1845\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1985\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1983\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[1;32m   1984\u001b[0m     size \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnrows \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_currow)\n\u001b[0;32m-> 1985\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread(nrows\u001b[39m=\u001b[39msize)\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m         nrows\n\u001b[1;32m   1925\u001b[0m     )\n\u001b[1;32m   1926\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:863\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    t_start = time()\n",
    "\n",
    "    df = next(df_iter)\n",
    "\n",
    "\n",
    "    df.to_sql(name='yellow_taxi_data' ,con=engine , if_exists='append')\n",
    "\n",
    "    t_end = time()\n",
    "\n",
    "    print('Inserted another chunk........, took %.3f seconds' % (t_end - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = 'yellow_tripdata_2024_01.csv'\n",
    "\n",
    "#empty_list = []\n",
    "#for month in ['%0.2d' % i for i in range(1, 13)]:\n",
    "result = wget.download(f'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet')\n",
    "\n",
    "parquet_table = pq.read_table(result)\n",
    "df = parquet_table.to_pandas()\n",
    "\n",
    "\n",
    "#empty_list.append(df)\n",
    "\n",
    "#merged_table = pd.concat(empty_list)\n",
    "#merged_table.to_csv(output_file_path, index=False)\n",
    "\n",
    "df.to_csv(output_file_path , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql(name='yellow_taxi_data', con= engine, if_exists='replace',index=False, chunksize=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine \n",
    "from sqlalchemy.exc import OperationalError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(params):\n",
    "    try:\n",
    "            \n",
    "        user = params.user\n",
    "        password = params.password\n",
    "        host = params.host\n",
    "        port = params.port\n",
    "        db = params.db\n",
    "        table_name = params.table_name\n",
    "        url=\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\"\n",
    "\n",
    "\n",
    "        #result = os.system(f\"wget {url} \")\n",
    "\n",
    "        output_file_path_csv = 'yellow_tripdata_2024.csv'\n",
    "\n",
    "        # Get the current directory\n",
    "        current_dir = os.getcwd()\n",
    "\n",
    "    # Set the output file path in the current directory\n",
    "        output_file_path = os.path.join(current_dir, \"yellow_tripdata_2024.parquet\")\n",
    "\n",
    "\n",
    "        #result = wget.download(f'{url}')\n",
    "\n",
    "        wget.download(url , output_file_path)\n",
    "\n",
    "        df = pd.read_parquet(output_file_path)\n",
    "\n",
    "\n",
    "        df.to_csv(output_file_path_csv , index=False)\n",
    "\n",
    "        df_iter = pd.read_csv(output_file_path_csv, iterator=True , chunksize=100000)\n",
    "        \n",
    "        df = next(df_iter)\n",
    "\n",
    "\n",
    "        engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\n",
    "        # Test connection\n",
    "        with engine.connect() as conn:\n",
    "            print(\"Database connection successful!\")\n",
    "    \n",
    "        df.head(n=0).to_sql(name= table_name ,con=engine , if_exists='replace' )\n",
    "\n",
    "\n",
    "        df.to_sql(name=table_name ,con=engine , if_exists='append')\n",
    "    \n",
    "    except OperationalError as e:\n",
    "        print(f\"Failed to connect to database: {e}\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/pawanchahar/Data-Engineering-Zoomcamp/Week_1_basics_n_setup/2_docker_sql/yellow_tripdata_2024 (1).parquet'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "url=\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-01.parquet\"\n",
    "\n",
    "\n",
    "#result = os.system(f\"wget {url} \")\n",
    "\n",
    "output_file_path_csv = 'yellow_tripdata_2024.csv'\n",
    "\n",
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Set the output file path in the current directory\n",
    "output_file_path = os.path.join(current_dir, \"yellow_tripdata_2024.parquet\")\n",
    "\n",
    "\n",
    "#result = wget.download(f'{url}')\n",
    "\n",
    "wget.download(url , output_file_path)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_parquet(output_file_path)\n",
    "\n",
    "\n",
    "df.to_csv(output_file_path_csv , index=False)\n",
    "\n",
    "df_iter = pd.read_csv(output_file_path_csv, iterator=True , chunksize=100000)\n",
    "\n",
    "df = next(df_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\n",
    "\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n",
    "# Test connection\n",
    "#with engine.connect() as conn:\n",
    "   # print(\"Database connection successful!\")\n",
    "\n",
    "table_name='yellow_taxi_trips'\n",
    "\n",
    "df.head(n=0).to_sql(name= table_name ,con=engine , if_exists='replace' )\n",
    "\n",
    "\n",
    "#df.to_sql(name=table_name ,con=engine , if_exists='append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted another chunk........, took 14.729 seconds\n",
      "Inserted another chunk........, took 12.767 seconds\n",
      "Inserted another chunk........, took 12.107 seconds\n",
      "Inserted another chunk........, took 12.046 seconds\n",
      "Inserted another chunk........, took 12.261 seconds\n",
      "Inserted another chunk........, took 11.588 seconds\n",
      "Inserted another chunk........, took 12.138 seconds\n",
      "Inserted another chunk........, took 17.646 seconds\n",
      "Inserted another chunk........, took 12.385 seconds\n",
      "Inserted another chunk........, took 11.799 seconds\n",
      "Inserted another chunk........, took 16.329 seconds\n",
      "Inserted another chunk........, took 11.536 seconds\n",
      "Inserted another chunk........, took 11.515 seconds\n",
      "Inserted another chunk........, took 11.861 seconds\n",
      "Inserted another chunk........, took 11.906 seconds\n",
      "Inserted another chunk........, took 11.421 seconds\n",
      "Inserted another chunk........, took 11.812 seconds\n",
      "Inserted another chunk........, took 11.551 seconds\n",
      "Inserted another chunk........, took 11.835 seconds\n",
      "Inserted another chunk........, took 12.407 seconds\n",
      "Inserted another chunk........, took 12.384 seconds\n",
      "Inserted another chunk........, took 13.842 seconds\n",
      "Inserted another chunk........, took 12.206 seconds\n",
      "Inserted another chunk........, took 11.698 seconds\n",
      "Inserted another chunk........, took 11.995 seconds\n",
      "Inserted another chunk........, took 11.638 seconds\n",
      "Inserted another chunk........, took 12.004 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kp/_4404mkx1zn_85zd51sfg3440000gn/T/ipykernel_4367/2263834719.py:5: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = next(df_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted another chunk........, took 10.604 seconds\n",
      "Inserted another chunk........, took 6.909 seconds\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     t_start \u001b[39m=\u001b[39m time()\n\u001b[0;32m----> 5\u001b[0m     df \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(df_iter)\n\u001b[1;32m      7\u001b[0m     df\u001b[39m.\u001b[39mto_sql(name\u001b[39m=\u001b[39mtable_name, con\u001b[39m=\u001b[39m engine, if_exists\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mreplace\u001b[39m\u001b[39m'\u001b[39m,index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m     t_end \u001b[39m=\u001b[39m time()\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1843\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1841\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m   1842\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1843\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_chunk()\n\u001b[1;32m   1844\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m   1845\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1985\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1983\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[1;32m   1984\u001b[0m     size \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnrows \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_currow)\n\u001b[0;32m-> 1985\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread(nrows\u001b[39m=\u001b[39msize)\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m         nrows\n\u001b[1;32m   1925\u001b[0m     )\n\u001b[1;32m   1926\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39mread_low_memory(nrows)\n\u001b[1;32m    235\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:863\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "\n",
    "    t_start = time()\n",
    "\n",
    "    df = next(df_iter)\n",
    "\n",
    "    df.to_sql(name=table_name, con= engine, if_exists='replace',index=False)\n",
    "\n",
    "    t_end = time()\n",
    "\n",
    "    print('Inserted another chunk........, took %.3f seconds' % (t_end - t_start))\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=\"localhost\",\n",
    "    database=\"ny_taxi\",\n",
    "    user=\"root\",\n",
    "    password=\"root\",\n",
    "    port=5432\n",
    ")\n",
    "print(\"Connection successful!\")\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection successful!\n"
     ]
    }
   ],
   "source": [
    "#engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\n",
    "\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n",
    "        \n",
    "        # Test connection\n",
    "with engine.connect() as conn:\n",
    "    print(\"Database connection successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
